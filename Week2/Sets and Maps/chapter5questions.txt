1. Hash codes are of type ‘int’ in Python. See code below.

!# bin/usr/env python

x = “John Doe”
y = hash(x)

print(y)
print(type(y))


2. A hash code gets converted into a value that can be used in a hash table by making use of the mod (%) operand. 
We accomplish this by taking the hash value of an object and using the mod operand to obtain the index in which 
the object will get stored at.


3. Once we have found the proper location we must check if the item is in the location. 
If it is, we return True. If not, we keep increasing the location hashed by one. 
If we reach a ‘None’ value, the item we are looking for is not in the hash table.


4. Collision strategies are needed when working with a hash table because different keys may hash to the same value, and we still want to provide a way to store them in the hash table. 
For example the key 12 and key 22 may hash to the same value in a table of size 10. 
For that, we use collision strategies to find a new location in the hash table for the key.


5. A map is a data structure that unlike a set, allows us to store relations between keys and values.
For example, the keys may be students ids and their respective values may be their name. 
A set only stores unique values within it and does not provide any way to map a key to a value.
A implication of a map that people may not be aware is that if you add a pre existent key in the map
with a new value that key:value pair gets overwritten. This is not a problem with sets since we only
care about the value itself and adding the same item just means overwriting with the same value.


6. If we turned things around, the add and membership methods would take less lines of code. By using a dictionary
to implement a set, the key could be the item to be added and the value could be its hash code, or anything honestly. Since
dictionaries do not allow repeated keys, this would maintain the set principle of unique values. When adding to the set, we could
simply write: hashSet[item] = hash(item). For the membership methods we could check if an item is in the set
by writing: item in hashSet, which would return True or False.


7. The load factor helps us obtain a O(1) amortized time complexity. 
If the load factor is above a certain threshold we must rehash our table to a new table of bigger size.
This technique allows us to decrease the load factor in the new rehashed table, which means that less 
collisions are likely to happen when adding new items and so forth. This operation is of amortized complexity O(1).


8. Rehashing is a technique used when the load factor of a table gets too big and it is likely for collisions to occur. 
Rehashing refers to transferring items into a new table of bigger size (double the size for example). 
This is accomplished by hashing the values in the previous table again since the new table is of a bigger size, 
and the newly hashed values may differ from before.


9. Memoization is an effective programming technique when dealing with functions that get called more than once with the same arguments. 
That is usually done through the use of a hash table that allows us to store information of the parameter passed to the function and its return value. 
As the recursive call moves toward the base case, memoization saves computational time of calling the same function over and over again increasing the
run-time stack, and instead looks up the value in the hash table. Since hash tables look up are O(1) time complexity, 
this is an extremely useful programming technique.


10. False. Memoization would not make the factorial function itself run faster, but it would make subsequent calls to the function run faster. See code below: 

memo = {}

def factorial_memo(n):
   if n in memo:
       return memo[n]


   if n == 1:
       memo[1] = 1
       return 1
  
   val = factorial_memo(n-1) * n
   memo[n] = val


   return val


s2 = time.time()
factorial_memo(900)
e2 = time.time()
print(e2-s2)


s3 = time.time()
factorial_memo(900)
e3 = time.time()
print(e3-s3)


In the first call to factorial_memo it’s time complexity is O(n) since we make n calls to the function.
This is the same time complexity without using memoization. 
On the other hand, the second call to factorial_memo is of O(1) time complexity, since the value passed 
as parameter has already been added to the hash table ‘memo’ it's lookup time is O(1).